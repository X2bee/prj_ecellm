{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForMaskedLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "hgf_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(hgf_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(hgf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "hgf_path = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(hgf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.train.zip: 1.78GB [12:21, 2.40MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /workspace/Korpora/namuwikitext/namuwikitext_20200302.train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.test.zip: 9.14MB [00:02, 3.13MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /workspace/Korpora/namuwikitext/namuwikitext_20200302.test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[namuwikitext] download namuwikitext_20200302.dev.zip: 8.71MB [00:03, 2.77MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip /workspace/Korpora/namuwikitext/namuwikitext_20200302.dev\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\"kowikitext\", root_dir=\"/workspace/Korpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Korpora.load(\"kowikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50368"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exaone_tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìķĦëĭĪ', 'ĠìĻľ', 'ìĿ´ëŁ¬', 'ëĥĲ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exaone_tokenizer.tokenize(\"아니 왜이러냐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ëĭ¤', 'Ġcartridge', 'Ġpiece', 'Ġterrestrial', 'Neil', 'Ġtread', 'ĠĠĠĠĠĠĠĠ', 'grass', 'Ġhold', '755', ')/((', 'ĠHH', 'Ġpercentile', 'ĠCunningham', 'Ġsisters', 'åĬł', 'Ġunt', 'ette', \"'},\", 'avier']\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "tokens = list(vocab.keys())\n",
    "\n",
    "# 디코딩 후 출력\n",
    "decoded_tokens = [tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(token)) for token in tokens]\n",
    "print(decoded_tokens[:20])  # 일부만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokenizer_file': 'tokenizer.json', 'vocab_file': 'tokenizer.model'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_files_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exaone_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "50284 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# To get predictions for the mask:\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m masked_index \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m predicted_token_id \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m, masked_index]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(predicted_token_id)\n",
      "\u001b[1;31mValueError\u001b[0m: 50284 is not in list"
     ]
    }
   ],
   "source": [
    "text = \"한글 처리는 어떻게 [Mask].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
