{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"x2bee/KoModernBERT-base-mlm-v02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ë¬¸',\n",
       " 'ìŀ¥',\n",
       " 'ìĿĦ',\n",
       " 'ĠìķĦë¬´',\n",
       " 'ê±°',\n",
       " 'ëĤ',\n",
       " 'ĺ',\n",
       " 'Ġ',\n",
       " 'íķľêµŃìĸ´',\n",
       " 'Ġì²ĺë¦¬',\n",
       " 'ê°Ģ',\n",
       " 'Ġê°ĢëĬ¥',\n",
       " 'íķľì§Ģ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = tokenizer.tokenize(\"문장을 아무거나 한국어 처리가 가능한지\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idxs = tokenizer.convert_tokens_to_ids(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문장을 아무거나 한국어 처리가 가능한지'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"BBPE 토크나이저는 토큰을 Byte 형식으로 나눕니다.\")\n",
    "# Result\n",
    "['BB', 'PE', 'ĠíĨłíģ¬', 'ëĤ', 'ĺ', 'ìĿ´ìłĢ', 'ëĬĶ', 'ĠíĨłíģ°', 'ìĿĦ', 'ĠByte', 'ĠíĺķìĭĿ', 'ìľ¼', 'ë¡ľ', 'ĠëĤĺ', 'ëĪķ', 'ëĭĪëĭ¤', '.']\n",
    "tokenizer.covert_tokens_to_ids(['BB', 'PE', 'ĠíĨłíģ¬', 'ëĤ', 'ĺ', 'ìĿ´ìłĢ', 'ëĬĶ', 'ĠíĨłíģ°', 'ìĿĦ', 'ĠByte', 'ĠíĺķìĭĿ', 'ìľ¼', 'ë¡ľ', 'ĠëĤĺ', 'ëĪķ', 'ëĭĪëĭ¤', '.'])\n",
    "# Result\n",
    "[10172, 3246, 58998, 44028, 235, 98284, 24169, 59366, 28736, 24128, 54116, 51745, 35296, 50641, 76029, 31912, 15]\n",
    "tokenizer.decode([10172, 3246, 58998, 44028, 235, 98284, 24169, 59366, 28736, 24128, 54116, 51745, 35296, 50641, 76029, 31912, 15])\n",
    "# Result\n",
    "'BBPE 토크나이저는 토큰을 Byte 형식으로 나눕니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameter-free poolers to get the sentence embedding\n",
    "    'cls': [CLS] representation with BERT/RoBERTa's MLP pooler.\n",
    "    'cls_before_pooler': [CLS] representation without the original MLP pooler.\n",
    "    'avg': average of the last layers' hidden states at each token.\n",
    "    'avg_top2': average of the last two layers.\n",
    "    'avg_first_last': average of the first and the last layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, pooler_type):\n",
    "        super().__init__()\n",
    "        self.pooler_type = pooler_type\n",
    "        assert self.pooler_type in [\"cls\", \"cls_before_pooler\", \"avg\", \"avg_top2\", \"avg_first_last\"], \"unrecognized pooling type %s\" % self.pooler_type\n",
    "\n",
    "    def forward(self, attention_mask, outputs):\n",
    "        last_hidden = outputs.last_hidden_state\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "        if self.pooler_type in ['cls_before_pooler', 'cls']:\n",
    "            return last_hidden[:, 0]\n",
    "        elif self.pooler_type == \"avg\":\n",
    "            return ((last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1))\n",
    "        elif self.pooler_type == \"avg_first_last\":\n",
    "            first_hidden = hidden_states[1]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        elif self.pooler_type == \"avg_top2\":\n",
    "            second_last_hidden = hidden_states[-2]\n",
    "            last_hidden = hidden_states[-1]\n",
    "            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n",
    "            return pooled_result\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class ModernBERTEmbedding:\n",
    "    def __init__(self, model, tokenizer, pooler_type=\"avg\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.pooler = Pooler(pooler_type)\n",
    "\n",
    "    def encode(self, inputs, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        768차원의 임베딩 벡터 추출\n",
    "        \"\"\"\n",
    "        self.model.to(device)\n",
    "\n",
    "        # 입력 데이터를 모델에 전달\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Pooling 수행\n",
    "        embeddings = self.pooler(inputs[\"attention_mask\"], outputs)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import ModernBertModel, AutoTokenizer\n",
    "\n",
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"CocoRoF/KMB_SimCSE_test\")\n",
    "model = ModernBertModel.from_pretrained(\"CocoRoF/KMB_SimCSE_test\")\n",
    "\n",
    "embeddings_model = ModernBERTEmbedding(model, tokenizer)\n",
    "# Tokenize input texts\n",
    "texts = [\n",
    "    \"아니 이거 잘되야 하는데 .. 그렇지?\",\n",
    "    \"잘되야 되는건 뻔한 말이고.\",\n",
    "    \"잘 될거야.\"\n",
    "]\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = embeddings_model.encode(inputs, device=\"cuda\")  # inputs는 딕셔너리 형태\n",
    "\n",
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[1], cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (texts[0], texts[2], cosine_sim_0_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
