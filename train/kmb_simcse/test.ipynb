{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/train/kmb_simcse\")\n",
    "\n",
    "from model import ModernBERTSimCSE\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, Trainer, get_scheduler, TrainingArguments, ModernBertForMaskedLM, ModernBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = ModernBERTSimCSE.from_pretrained(\n",
    "    \"x2bee/KoModernBERT-base-mlm-v03-retry-ckp03\",\n",
    "    subfolder=\"last-checkpoint\",\n",
    "    pooler_type=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertConfig {\n",
       "  \"_name_or_path\": \"x2bee/KoModernBERT-base-mlm-v03-retry-ckp03\",\n",
       "  \"architectures\": [\n",
       "    \"ModernBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 50281,\n",
       "  \"classifier_activation\": \"gelu\",\n",
       "  \"classifier_bias\": false,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"classifier_pooling\": \"mean\",\n",
       "  \"cls_token_id\": 50281,\n",
       "  \"decoder_bias\": true,\n",
       "  \"deterministic_flash_attn\": false,\n",
       "  \"embedding_dropout\": 0.0,\n",
       "  \"eos_token_id\": 50282,\n",
       "  \"global_attn_every_n_layers\": 3,\n",
       "  \"global_rope_theta\": 160000.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_activation\": \"gelu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_cutoff_factor\": 2.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1152,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"local_attention\": 128,\n",
       "  \"local_rope_theta\": 10000.0,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mlp_dropout\": 0.0,\n",
       "  \"model_type\": \"modernbert\",\n",
       "  \"norm_bias\": false,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"pad_token_id\": 50283,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"reference_compile\": false,\n",
       "  \"repad_logits_with_grad\": false,\n",
       "  \"sep_token_id\": 50282,\n",
       "  \"sparse_pred_ignore_index\": -100,\n",
       "  \"sparse_prediction\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.48.0.dev0\",\n",
       "  \"vocab_size\": 95663\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBERTSimCSE(\n",
       "  (modernbert): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(95663, 768, padding_idx=50283)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): Pooler()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at CocoRoF/KMB_SimCSE_test were not used when initializing ModernBertModel: ['modernbert.embeddings.norm.weight', 'modernbert.embeddings.tok_embeddings.weight', 'modernbert.final_norm.weight', 'modernbert.layers.0.attn.Wo.weight', 'modernbert.layers.0.attn.Wqkv.weight', 'modernbert.layers.0.mlp.Wi.weight', 'modernbert.layers.0.mlp.Wo.weight', 'modernbert.layers.0.mlp_norm.weight', 'modernbert.layers.1.attn.Wo.weight', 'modernbert.layers.1.attn.Wqkv.weight', 'modernbert.layers.1.attn_norm.weight', 'modernbert.layers.1.mlp.Wi.weight', 'modernbert.layers.1.mlp.Wo.weight', 'modernbert.layers.1.mlp_norm.weight', 'modernbert.layers.10.attn.Wo.weight', 'modernbert.layers.10.attn.Wqkv.weight', 'modernbert.layers.10.attn_norm.weight', 'modernbert.layers.10.mlp.Wi.weight', 'modernbert.layers.10.mlp.Wo.weight', 'modernbert.layers.10.mlp_norm.weight', 'modernbert.layers.11.attn.Wo.weight', 'modernbert.layers.11.attn.Wqkv.weight', 'modernbert.layers.11.attn_norm.weight', 'modernbert.layers.11.mlp.Wi.weight', 'modernbert.layers.11.mlp.Wo.weight', 'modernbert.layers.11.mlp_norm.weight', 'modernbert.layers.12.attn.Wo.weight', 'modernbert.layers.12.attn.Wqkv.weight', 'modernbert.layers.12.attn_norm.weight', 'modernbert.layers.12.mlp.Wi.weight', 'modernbert.layers.12.mlp.Wo.weight', 'modernbert.layers.12.mlp_norm.weight', 'modernbert.layers.13.attn.Wo.weight', 'modernbert.layers.13.attn.Wqkv.weight', 'modernbert.layers.13.attn_norm.weight', 'modernbert.layers.13.mlp.Wi.weight', 'modernbert.layers.13.mlp.Wo.weight', 'modernbert.layers.13.mlp_norm.weight', 'modernbert.layers.14.attn.Wo.weight', 'modernbert.layers.14.attn.Wqkv.weight', 'modernbert.layers.14.attn_norm.weight', 'modernbert.layers.14.mlp.Wi.weight', 'modernbert.layers.14.mlp.Wo.weight', 'modernbert.layers.14.mlp_norm.weight', 'modernbert.layers.15.attn.Wo.weight', 'modernbert.layers.15.attn.Wqkv.weight', 'modernbert.layers.15.attn_norm.weight', 'modernbert.layers.15.mlp.Wi.weight', 'modernbert.layers.15.mlp.Wo.weight', 'modernbert.layers.15.mlp_norm.weight', 'modernbert.layers.16.attn.Wo.weight', 'modernbert.layers.16.attn.Wqkv.weight', 'modernbert.layers.16.attn_norm.weight', 'modernbert.layers.16.mlp.Wi.weight', 'modernbert.layers.16.mlp.Wo.weight', 'modernbert.layers.16.mlp_norm.weight', 'modernbert.layers.17.attn.Wo.weight', 'modernbert.layers.17.attn.Wqkv.weight', 'modernbert.layers.17.attn_norm.weight', 'modernbert.layers.17.mlp.Wi.weight', 'modernbert.layers.17.mlp.Wo.weight', 'modernbert.layers.17.mlp_norm.weight', 'modernbert.layers.18.attn.Wo.weight', 'modernbert.layers.18.attn.Wqkv.weight', 'modernbert.layers.18.attn_norm.weight', 'modernbert.layers.18.mlp.Wi.weight', 'modernbert.layers.18.mlp.Wo.weight', 'modernbert.layers.18.mlp_norm.weight', 'modernbert.layers.19.attn.Wo.weight', 'modernbert.layers.19.attn.Wqkv.weight', 'modernbert.layers.19.attn_norm.weight', 'modernbert.layers.19.mlp.Wi.weight', 'modernbert.layers.19.mlp.Wo.weight', 'modernbert.layers.19.mlp_norm.weight', 'modernbert.layers.2.attn.Wo.weight', 'modernbert.layers.2.attn.Wqkv.weight', 'modernbert.layers.2.attn_norm.weight', 'modernbert.layers.2.mlp.Wi.weight', 'modernbert.layers.2.mlp.Wo.weight', 'modernbert.layers.2.mlp_norm.weight', 'modernbert.layers.20.attn.Wo.weight', 'modernbert.layers.20.attn.Wqkv.weight', 'modernbert.layers.20.attn_norm.weight', 'modernbert.layers.20.mlp.Wi.weight', 'modernbert.layers.20.mlp.Wo.weight', 'modernbert.layers.20.mlp_norm.weight', 'modernbert.layers.21.attn.Wo.weight', 'modernbert.layers.21.attn.Wqkv.weight', 'modernbert.layers.21.attn_norm.weight', 'modernbert.layers.21.mlp.Wi.weight', 'modernbert.layers.21.mlp.Wo.weight', 'modernbert.layers.21.mlp_norm.weight', 'modernbert.layers.3.attn.Wo.weight', 'modernbert.layers.3.attn.Wqkv.weight', 'modernbert.layers.3.attn_norm.weight', 'modernbert.layers.3.mlp.Wi.weight', 'modernbert.layers.3.mlp.Wo.weight', 'modernbert.layers.3.mlp_norm.weight', 'modernbert.layers.4.attn.Wo.weight', 'modernbert.layers.4.attn.Wqkv.weight', 'modernbert.layers.4.attn_norm.weight', 'modernbert.layers.4.mlp.Wi.weight', 'modernbert.layers.4.mlp.Wo.weight', 'modernbert.layers.4.mlp_norm.weight', 'modernbert.layers.5.attn.Wo.weight', 'modernbert.layers.5.attn.Wqkv.weight', 'modernbert.layers.5.attn_norm.weight', 'modernbert.layers.5.mlp.Wi.weight', 'modernbert.layers.5.mlp.Wo.weight', 'modernbert.layers.5.mlp_norm.weight', 'modernbert.layers.6.attn.Wo.weight', 'modernbert.layers.6.attn.Wqkv.weight', 'modernbert.layers.6.attn_norm.weight', 'modernbert.layers.6.mlp.Wi.weight', 'modernbert.layers.6.mlp.Wo.weight', 'modernbert.layers.6.mlp_norm.weight', 'modernbert.layers.7.attn.Wo.weight', 'modernbert.layers.7.attn.Wqkv.weight', 'modernbert.layers.7.attn_norm.weight', 'modernbert.layers.7.mlp.Wi.weight', 'modernbert.layers.7.mlp.Wo.weight', 'modernbert.layers.7.mlp_norm.weight', 'modernbert.layers.8.attn.Wo.weight', 'modernbert.layers.8.attn.Wqkv.weight', 'modernbert.layers.8.attn_norm.weight', 'modernbert.layers.8.mlp.Wi.weight', 'modernbert.layers.8.mlp.Wo.weight', 'modernbert.layers.8.mlp_norm.weight', 'modernbert.layers.9.attn.Wo.weight', 'modernbert.layers.9.attn.Wqkv.weight', 'modernbert.layers.9.attn_norm.weight', 'modernbert.layers.9.mlp.Wi.weight', 'modernbert.layers.9.mlp.Wo.weight', 'modernbert.layers.9.mlp_norm.weight']\n",
      "- This IS expected if you are initializing ModernBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ModernBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ModernBertModel were not initialized from the model checkpoint at CocoRoF/KMB_SimCSE_test and are newly initialized: ['embeddings.norm.weight', 'embeddings.tok_embeddings.weight', 'final_norm.weight', 'layers.0.attn.Wo.weight', 'layers.0.attn.Wqkv.weight', 'layers.0.mlp.Wi.weight', 'layers.0.mlp.Wo.weight', 'layers.0.mlp_norm.weight', 'layers.1.attn.Wo.weight', 'layers.1.attn.Wqkv.weight', 'layers.1.attn_norm.weight', 'layers.1.mlp.Wi.weight', 'layers.1.mlp.Wo.weight', 'layers.1.mlp_norm.weight', 'layers.10.attn.Wo.weight', 'layers.10.attn.Wqkv.weight', 'layers.10.attn_norm.weight', 'layers.10.mlp.Wi.weight', 'layers.10.mlp.Wo.weight', 'layers.10.mlp_norm.weight', 'layers.11.attn.Wo.weight', 'layers.11.attn.Wqkv.weight', 'layers.11.attn_norm.weight', 'layers.11.mlp.Wi.weight', 'layers.11.mlp.Wo.weight', 'layers.11.mlp_norm.weight', 'layers.12.attn.Wo.weight', 'layers.12.attn.Wqkv.weight', 'layers.12.attn_norm.weight', 'layers.12.mlp.Wi.weight', 'layers.12.mlp.Wo.weight', 'layers.12.mlp_norm.weight', 'layers.13.attn.Wo.weight', 'layers.13.attn.Wqkv.weight', 'layers.13.attn_norm.weight', 'layers.13.mlp.Wi.weight', 'layers.13.mlp.Wo.weight', 'layers.13.mlp_norm.weight', 'layers.14.attn.Wo.weight', 'layers.14.attn.Wqkv.weight', 'layers.14.attn_norm.weight', 'layers.14.mlp.Wi.weight', 'layers.14.mlp.Wo.weight', 'layers.14.mlp_norm.weight', 'layers.15.attn.Wo.weight', 'layers.15.attn.Wqkv.weight', 'layers.15.attn_norm.weight', 'layers.15.mlp.Wi.weight', 'layers.15.mlp.Wo.weight', 'layers.15.mlp_norm.weight', 'layers.16.attn.Wo.weight', 'layers.16.attn.Wqkv.weight', 'layers.16.attn_norm.weight', 'layers.16.mlp.Wi.weight', 'layers.16.mlp.Wo.weight', 'layers.16.mlp_norm.weight', 'layers.17.attn.Wo.weight', 'layers.17.attn.Wqkv.weight', 'layers.17.attn_norm.weight', 'layers.17.mlp.Wi.weight', 'layers.17.mlp.Wo.weight', 'layers.17.mlp_norm.weight', 'layers.18.attn.Wo.weight', 'layers.18.attn.Wqkv.weight', 'layers.18.attn_norm.weight', 'layers.18.mlp.Wi.weight', 'layers.18.mlp.Wo.weight', 'layers.18.mlp_norm.weight', 'layers.19.attn.Wo.weight', 'layers.19.attn.Wqkv.weight', 'layers.19.attn_norm.weight', 'layers.19.mlp.Wi.weight', 'layers.19.mlp.Wo.weight', 'layers.19.mlp_norm.weight', 'layers.2.attn.Wo.weight', 'layers.2.attn.Wqkv.weight', 'layers.2.attn_norm.weight', 'layers.2.mlp.Wi.weight', 'layers.2.mlp.Wo.weight', 'layers.2.mlp_norm.weight', 'layers.20.attn.Wo.weight', 'layers.20.attn.Wqkv.weight', 'layers.20.attn_norm.weight', 'layers.20.mlp.Wi.weight', 'layers.20.mlp.Wo.weight', 'layers.20.mlp_norm.weight', 'layers.21.attn.Wo.weight', 'layers.21.attn.Wqkv.weight', 'layers.21.attn_norm.weight', 'layers.21.mlp.Wi.weight', 'layers.21.mlp.Wo.weight', 'layers.21.mlp_norm.weight', 'layers.3.attn.Wo.weight', 'layers.3.attn.Wqkv.weight', 'layers.3.attn_norm.weight', 'layers.3.mlp.Wi.weight', 'layers.3.mlp.Wo.weight', 'layers.3.mlp_norm.weight', 'layers.4.attn.Wo.weight', 'layers.4.attn.Wqkv.weight', 'layers.4.attn_norm.weight', 'layers.4.mlp.Wi.weight', 'layers.4.mlp.Wo.weight', 'layers.4.mlp_norm.weight', 'layers.5.attn.Wo.weight', 'layers.5.attn.Wqkv.weight', 'layers.5.attn_norm.weight', 'layers.5.mlp.Wi.weight', 'layers.5.mlp.Wo.weight', 'layers.5.mlp_norm.weight', 'layers.6.attn.Wo.weight', 'layers.6.attn.Wqkv.weight', 'layers.6.attn_norm.weight', 'layers.6.mlp.Wi.weight', 'layers.6.mlp.Wo.weight', 'layers.6.mlp_norm.weight', 'layers.7.attn.Wo.weight', 'layers.7.attn.Wqkv.weight', 'layers.7.attn_norm.weight', 'layers.7.mlp.Wi.weight', 'layers.7.mlp.Wo.weight', 'layers.7.mlp_norm.weight', 'layers.8.attn.Wo.weight', 'layers.8.attn.Wqkv.weight', 'layers.8.attn_norm.weight', 'layers.8.mlp.Wi.weight', 'layers.8.mlp.Wo.weight', 'layers.8.mlp_norm.weight', 'layers.9.attn.Wo.weight', 'layers.9.attn.Wqkv.weight', 'layers.9.attn_norm.weight', 'layers.9.mlp.Wi.weight', 'layers.9.mlp.Wo.weight', 'layers.9.mlp_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=\"CocoRoF/KMB_SimCSE_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[99.9057]], device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tokenizer로 데이터를 토큰화\n",
    "result = tokenizer(\n",
    "    \"sentence1\",\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"  # PyTorch 텐서로 반환\n",
    ")\n",
    "\n",
    "# GPU로 텐서 이동 (CUDA 디바이스 사용 가능 여부 확인)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "result = {key: value.to(device) for key, value in result.items()}\n",
    "model = model.to(device)  # 모델도 GPU로 이동\n",
    "\n",
    "# 모델에 입력\n",
    "anchor_pooler = model(\n",
    "    input_ids=result['input_ids'],\n",
    "    attention_mask=result['attention_mask'],\n",
    "    return_dict=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutput' object has no attribute 'pooler_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manchor_pooler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooler_output\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutput' object has no attribute 'pooler_output'"
     ]
    }
   ],
   "source": [
    "anchor_pooler.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_pooler[0][:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BM-K/KoSimCSE-roberta-multitask\")\n",
    "modelmb = AutoModel.from_pretrained(\"BM-K/KoSimCSE-roberta-multitask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
